 Entonces, estábamos viendo lo que es este cuaderno, que es el primero que coloque en el grupo, estuyéndome en la mitad, porcaritábamos ya, pues, a comienzo desde el inicio. Pero si quería comenzar como seguramente lo que ya han visto, y si no lo han visto tampoco hay problema, es decir, si ustedes en algún punto quisieran repasar, eso lo vamos a hacer en ejercicio. Yo creo que la siguiente sesión, porque para poder introducir la regularización de redes neuronales tenemos que ver, porque en algunos casos de uso son mejores que los modelos clásicos. Entonces, como muy seguramente ustedes ya saben Python, o saben, Scikit Learn, que es como la capa más de alto nivel, es decir, menos código para la implementación de modelos, como Exhibust, pues que sale a todos y todo eso, pues podemos hacer pruebas muy rápido de todos estos modelos y una retenurona. En un caso que sepamos, digamos, ya por la experiencia, no sabe que las retenurinales tuvieron mejor resultados para poder comparar y también en las que no, para que podamos como ver la diferencia y que ganamos y que perdemos y que luego volvemos a recuperar, mediante los papers y en el que no lo hagas. Disto. Entonces, estábamos en la parte de nos supervisada, recuerden que normalmente los modelos que uno quiere hacer en la medida de lo posible y ojalá se tuvieran, pero normalmente esos muy costosos es hacer una aprendizaje supervisada, generar datos de ticat de ticatamiento puede ser tan costoso como un millón de dólares, depende de qué tipos de datos estemos hablando. Y muchas veces nosotros no tenemos las etiquetas. Entonces, cuando ocurre eso, tenemos que recurrir a la aprendizaje no supervisado, dependiendo de lo que estemos tratando de hacer también para poder hallar un patrón en los datos y quise tratar de generar etiquetas para luego volver a este mundo, ¿no? Pero pues poten tener también otros propósitos como hacer un class de niente de cual son esos universos de los datos, recuerden que finalmente todo esto funciona cuando somos capaces de transformar nuestro problema en, excuse me duequí to la, las uéreles para que podamos ver en full screen, para que el ámbito de todo esto es que nosotros podemos perfilar el fenómeno en entidades matemáticas. Ya sean números, ya sean categorías, ya sea información estructurada como textos, audios, pero si podemos traducir el problema a algunos variables numéricas o categoricas, entonces estamos en el mundo matemático donde todos estos procesos son posibles, es decir, a la final lo que queremos es transformar un elemento de nuestro estudio no amuestra en un punto de una dimensión alta muy seguramente que probablemente podamos visualizar, usamos algunos métodos de reducción de dimensión, que no sé si nos están en el pd, pero pues puede ser que sea un material de la de la signatura, los métodos de reducción de dimensión, porque también se usan redes neuronales para ese asunto, con algo que se llama autoencoders. Entonces la cosa es finalmente si nosotros podemos dibujar una nube de puntos, pues podemos hacer diferentes estrategias para poderlos clasificar, etiquetar, entonces aquí es como un perfilamiento. Facebook se o cuenta de eso hace mucho tiempo y por eso es que nos tienes segmentados en nichos y con 10 likes puedes saber más de usted mismo, más de usted que es un mamá, eso está en una charlatete luego se las paso y entonces es muy interesante porque si podemos traducir al mundo matemático todos los fenómenos que están tratando de desarrollar o investigar puestes paulosos, hay cosas que no se pueden hacer, por ejemplo la inteligencia humana todavía no se ha podido perfilar en vectores porque eso sería desarrollar de alguna manera la inteligencia artificial general y eso es algo que hay teobate, según Álvaro Montenegro el dice que faltan más o menos 50 años para lograrlo, Eupenea y todas estas empresas se han que en estos seis meses pero ahorita no se se han visto hay una crisis económica mundial porque las empresas prometieron mucho y parece que la saturación de la tecnología va a estar en los modelos, transformar, se decir que tienen que haber otra tecnología que alguien tiene que desarrollar en estos años para poderos acercar bien a la inteligencia artificial general de todas maneras ayer creo que salió un figure 2 que es un robot que ya habla y tiene articulación, esas cosas, todas esas cosas los vamos a ver ahorita si alcanzas el tiempo, entonces perfilamos traducimos al lenguaje matemático, podemos analizar, entonces en los métodos no supervisados cuando no tenemos las etiquetas, cuando no podemos hacer restos modelos de regresión o hay clasificación de ensencillo entonces recurrimos a métodos no supervisados donde podemos darnos como una especie de insight de que de que sus datos que que son como se agrupan que universos hay y está puesto hasta cada ma de modelos está el simins fusi el min shift el camins el de vez can el el closterina glomerativo gerárquico bueno hay un poco de cosas también están los métodos clásicos de regla de negocio de asociación de reglas explícitas programas una explícita dimensión de dimensión entonces por ahí uno que le gusta muchísimo en los estadísticos es el pca muy útil para algunos casos muy inútil para problemas no lineales pero definitivamente si usted tiene un problema lineal apliqué este porque interno de producción es lo que le va a servir de sne una técnica que particularmente no me agrada pero pues que hace parte es porque pues acá se generan muchos errores de interpretación y a que pues hay es un método de visualización y aquí faltan artísimos métodos de reducción dimensión falta un map falta pac map falta trim up faltan bastantes que pues si alguien está interesado me puede también preguntar y yo con mucho gusto le doy los repositorios y es normalmente su repositorio se está en la implementación directa de los métodos por ejemplo en un trabajo en el en fiscalía se busca por ejemplo perfilar algunos relatos algunos textos por ahí y lo que se veía era que naturalmente por contexto de los modelos de lengua de natural comenzan a aparecer unos clusters de temáticas que apuntaban a ciertos casos y entonces se puede hacer la categorización y los arquietivos de cada uno de los clusters usando la reducción de dimensión pac map entonces súper interesante con problemas lineales como las veces y cosas entonces todo ese mundo es el mundo de lo supervisado y de lo no supervisado adicionalmente a eso el machine learning pues cuya de finición veremos ahorita es qué podemos utilizar todos estos métodos y hacer modelos más robustos no significa mejores pero más robustos quiere decir que puento más decisiones más informadas más decisiones referido a la probabilidad por ejemplo de que un de que yo puedo aprecer un dato o la probabilidad de que esa categoría sea lo que ocurre a partir de la justé de parámetros del modelo y pues hay muchos métodos que se hacen para que se utilizan para hacer eso que llaman el sample modelos entonces ahí está la técnica stacking baggy y busting y ahí desalen los famosos modelos creo que en el chat hubo uno una persona que apunto súper bien a un tipo de modelo que no están conocido pero es muy interesante que se llama el cat busting sin estúimal ese método ahorita les muestro o no las siguientes clases el cat busting es súper bueno cuando yo tengo la mayoría de datos categoricos lo utilizamos en una hackathon en en en maitico en un grupo en el que estamos en el curso que que obtuve la certificación hace poquito y utilizamos todos los modelos posibles para poder predecir el nivel de satisfacción de las personas que van en el tren bala de Japón el que vamos de séptimo desafortunadamente nos dimos cuenta que las personas tienen granjas de computadores y comienzan a hacer ajuste y parámetros a la loja y las diferencias eran en la tercera disifra de sí mal y el cat busting era el modelo que nos funciona mejor este modelo es súper interesante para las personas que tengan un problema con muchos variables categoricas seguramente este modelo les va a funcionar bastante bien y bueno todos estos modelos busting pues que no los vamos a ver porque no hace parte de la aprendizaje profundo pero yo imagino que lo sabrán visto en su clase de modelación o digamos estadística básica o avanzada sino pues también no hay problema después van a dar el material para que puedan informarse dentro del diplomado pero no es no está en el templo del diplomado sino dentro del repositorio que se llama ciencia de datos ahí están especificados el exigú es mejor dicho les vamos a ver una vez para llenarlos de literatura y que sepan que pues que todas estas cosas pueden ser solventadas en este curso pero pues obviamente no nos podemos desverarse a temas que nos en aprendizaje profundo porque pues sería un poquito irresponsable sin embargo está este que ciencia de datos el diplomado en ciencia de datos y ahí están especificados cada uno de esos modelos entonces aquí tenemos también técnicas de reducción de dimension pero principalmente tenemos los mectodos descriptivos multivariados ese fue un tema que también el profesor el campolías pardo nos nos ha ayudado a hacer esas clases muy muy interesantes y por acá entonces comenzamos a ver todos los bagging los bussting y también unas cositas con sere de tiempo interesantes entonces si están interesados en los modelos clásicos que hacen parte de la aprendizaje automático este repositorio también puede ser una solución a eso cualquier cosa pues podrían preguntarme porque puede ser entre todos construimos esos de notebooks entonces yo debería estar en la capacidad de responder preguntas de alto nivel estadístico de lo que ustedes de pronto tengan dudas entonces fíjense que el machine learning es una cosa enorme la pregunta es entonces si está todos estos métodos porque necesitamos ir más allá cierto entonces la respuesta ocurrió porque los modelos en algún momento ya comenzaron a tener saturación igual que va a estar pasando seguramente con los modelos del lenguaje natural muy pronto entonces que ocurrió estos modelos que seguramente entonces la encuesta que vía que no quieren repasar rees neuronales desde cero bueno que dos siete cincuenta vamos a repasar lo pues digamos no es de cero sino un poquito más avanzado para que no sea pues tan aburrido para los que ya saben pero básicamente las rees neuronales son existentes de 1905 incluso antes y entonces la pregunta es por qué nos están utilizando desde esa época sí pues son tan potentes en esta hoy en este actualmente y la respuesta es porque no había recursos computacionales y dos no había una tecnología que ahorita les vamos a ver cuando conocemos en los bubudes del inicio y es que no existía algo que se llama la diferenciación automática y eso es algo que está implementado en todos los frameworks de inteligencia artificial o idea y básicamente si es un algoritmo que le permite hacer diferenciación automática y no tiene que preocuparse por la función de pérdida si no se han entendido el bocó claro que les estoy diciendo en este momento no se preocupen la siguiente decisión comenzaremos entonces formalmente con rees neuronales artificiales un poquito de aquí un poquito de allá y comenzamos ya con la regularización que está planteada y otros temitas que les queremos estar bueno entonces qué pasa el machine learning como se anun cuenta es una rama super grande y el deep learning no es nada más que una subramita chiquita de del machine learning es un sub conjunto a mí no me gusta mostrar mucho ese ese esas bolitas que están entonces machine learning y por dentro de deep learning porque siento que está esta categorización de mapa conceptual es mucho más clara el deep learning con las redes neuronales acá donde dice deep learning es clarísimo es una ramaita del machine learning que se especializa en si datos tipos de problemas ahora cuál es el secreto y porque funciona la cosa es que todos los métodos de machine learning que ustedes seguramente vieron tienen supuestos muy fuertes en muchos casos uno de los supuestos fuertes es que los datos tienen una distribución normal o hay algún proceso de normalidad para que los para que las problemas tengan solución única no todos por ejemplo random forest y todos los muertos buscintos son no lineas algunos son no lineales sin embargo tienen aspectos fuertes que hacen que los modelos sean un poquito rígidas entonces les voy a contar el secreto en este momento del porque el aprendizaje profundo que ahorita todos nos hemos visto la definición formal es diferente del machine learning y bueno es una especialización del machine learning que logró tener tanto éxito y todos se basa en un teorema matemático cuya demostración no vamos a ver pero que es muy interesante y se llama el teorema aproximación universal entonces llama universal la proximación y es este entonces vamos a colocarlo en a mi ya no me sirve este traductor yo lo utiliza esta red no lo utiliza para traducir pero parece que ya no ya no funciona entonces para fomentar el uso de la segunda lengua entonces vamos a leer en inglés entonces qué pasa vamos a como tal a la a la a la definición de al al teorema como tal no se asusten todo esto se puede traducir a cristiano decir a colombiano entonces según este teorema dice que cualquier función continua se puede aproximar mediante una red neuronal con suficientes capas es algo que parece un poco como loco pero es verdad y esto fue lo que realmente desde mi punto de esa matemático hizo la revolución no importa en los detalles no importa qué significa lepsilón lo importante es que si yo tengo una función continua también conocida como mis datos si ya vemos como así que los datos protestaron no cualquier conjunto de datos puede ser aproximado o modelado por una red neuronal y eso cambió toda la historia de la humanidad porque porque es que vamos a hacer un pequeño dibujito en el whiteboard de creo que este es el que son la y el dos siguientes entonces a que se refiere eso de próxima data si porque lepsilón y todo eso es que resulta que si usted tiene lo abajo de los dimensiones porque es que ya dibujar en más dimensiones es bueno artista pues imagínense que usted tiene unos datos así cierto unos datos que puede ser la final uno puede siempre dibujar los datos porque a pesar de que tenga datos categoricos o textuales uno siempre lo puede convertir en vectores o sea lo puede convertir en números en muchas dimensiones pero imagínense que estas dimensiones son muchas dimensiones y usted tiene esos datos entonces cuál es la diferencia con los modelos clásicos y por qué de learning todo resulta que este teorema aproximación universal dice lo siguiente dice que si usted es capaz entonces disculpen acá yo es que no me acuerdo cómo es esto pero con gamole yo creo que rojito que si ustedes capaz de construir una red neuronal lo suficientemente profunda entonces la retornada lo vamos a ver la siguiente decisión no se preocupen y usted es capaz de aproximar cualquier conjunto datos con error 0 que significa eso que usted puede hacer esto eso dice el teorema aproximación universal usted siempre puede aproximar cualquier conjunto datos eso es una verdadera por eso se llama aproximación universal entonces el problema de este teorema es que no le dice cómo sólo le dice que es posible como toma temática entonces es posible y como a no se descubren y por eso existen todas esas arquitectores de aprendizaje profundo pero la conclusión del teorema y lo que cambió la manidez como oiga no importa de verdad cualquier conjunto datos que tengan conocidas como funciones continuas también funciona para funciones discontinuas pero ya es otro tema que no vamos a tratar siempre es posible que usted construya un modelo requeren que esto es un modelo no yo hago mis preacciones ya sea inventan a por fuera un ventana por dentro si darte pues lo veremos cuando ya me avanzando siempre es posible construir un modelo y eso cambió todo porque entonces uno dice a ok entonces yo construye una arquitectura precisa siempre va a poder solucionar mi problema y entonces ese fue el inicio de decir bueno los modelos clásicos tienen unas formas específicas de aproximar los datos pero por ejemplo una regreso en lineal simple pues con el en esos términos sin clistas porque pues multivariado pues no no se alcance en la gráfica sería un estilo de esto entonces yo como no puedo torcer esas líneas lo suficientemente bien pues yo jamal voy a poder aproximar totalmente los datos entonces acá entran cuando yo digo puedo aproximar los datos entre otras otros con ser otras inquietudes como bueno pero pues si será bueno aproximar los datos a cero o seguramente toque se para un conjunto test como para ver si nos estoy sobre ajustando mis datos eso ocurre y entonces este tore malo que hizo fue como darle fe a las personas y estamos hablando del año 1980 de que era posible ahora la tecnología esto es un tore más matemático la tecnología no estaba y por eso entonces por lo hemos alcohárnito de qué fue lo que ocurrió después tuvimos que hacer mucha invención de tecnologías en hardware en software que hicieron posible entre otras esto que yo pudiese hacer diferenciación automática para que después fuera posible hacer algo que se llama al back propagation que es básicamente como ustedes vieron un poquito de reino neuronales como yo puedo distribuir la responsabilidad de los errores de mis preacciones a lo largo de una red compleja y suficientemente profunda y eso cambió la historia porque entonces ya básicamente estengo los datos tengo el optimizador puedo diferenciar automáticamente puedo ajustar parámetros de manera universal super cool ahora cuáles son los arquitecturas de las redes que me funcionan bueno y eso que les estoy diciendo entonces logró que yo me saliera de los modelos clásicos y que acosta al principio de la interpretación de los mismos pudiésemos aproximar cualquier conjunto de ads super bacana entonces ícados cuando yo digo bueno tengo más capas en mi retenerona lo hace la más gruesita voy a decir que tengo más parámetros entonces tengo posibilidad de decir que es como una especie de profundidad y de ahí la palabra aprendizaje profundo aprendizaje por qué porque al final pues en el término coloquial está dicho que las máquinas aprenden y pues no es que aprendan sino que apre uno ajusta los parámetros para poder hacer predicciones que sirven para algo aprendizaje profundo entonces sería la rama del machine learning que se encarga de las redes neuronales específicamente que tienen más de una capa de profundidad esa es digamos el versus que era como el objetivo esta sesión como yo me desliga un poquito de los modelos clásicos para tener otro tipo de modelos no quiere decir que la aprendizaje profundo entonces sea al todo como les digo no siempre debe hacer la evaluación de los modelos clásicos y de las redes neuronales para poder tomar una decisión de negocio no está bien que usted penta solo las redes neuronales pero tampoco está bien que las hoy entonces tiene que tener un equilibrio en decir yo tengo que ver todo listo entonces básicamente será como el objetivo de hoy más allá de eso entonces pues las protección de sesiones vamos a ver como las tecnologías involucradas entender que significa un tensor que son herramientas básicas para que podamos desarrollar la traducción de problemas a la tecnología de la red neuronales que muy seguramente ustedes vieron pero ya pues a nivel mucho más especializado a nivel de aprendizaje profundo vamos a ver un pronto un poquito sobre a moder de de repaso pues que se estue la red diferenciación automática que es esto de los optimizadores porque funcionan que es una función de pérdida que significa el gradientes en diente la tecnología de las componciones que significa eso y cómo lo aplico en algún problema específico es tu netamente para visión para computadora cambió la humanidad en el 2016 Alex net un modelo de Google y que pues básicamente yo puedo hacer modelos un poquito más robustos que los tradicionales porque al tener más parámetros pues yo puedo ajustar mucho mejor los datos gracias a este programa de aproximación universal cierto aplicaciones súper interesantes con el aprendizaje profundo color y a la imagen entonces hay que cambiar la perspectiva de la aprendizaje profundo ya no se trata de ver el dato en sí sino de entender el problema yo como organizó mis datos y que la carcasa sea la red neuronal profundo entonces aquí la entrada cambia es como bueno si yo tengo las fotos a color y las puedo volver blanco y negro pero si yo construye una red entrada que tenga las fotos en blanco y negro pero tengo las de color y entrenos en moda de los dos parámetros por dentro de ese de esa red profunda van a tener la noción de cómo coloriar pixels en blanco y negro una aplicación súper interesante de visión por computadora ahora pues todo esto de él me lo va a saltar porque pues no es una clase de redes neuronales y no introducción al inteligencia artificial aplicaciones por montones puedo convertir textos en vectores y si esos vectores están cerca entonces yo puedo alguna manera hacer un címil del cerebro humano donde diga los conceptos y mil ares están cerca matemáticamente hablando y por lo tanto todas esas operaciones lingüísticas son posibles luego chaty pití y todas esas otras tecnologías las tecnologías de base que son las GPUs y todo esto es para precisamente la aprendizad de profundo cuál es el secreto de la aprendizad de profundo la multiplicación de matrices entonces si vieron al jebre al lineal muy seguramente esto lo que hace unos chips multiplica matrices en paralelo y ese es el secreto de la aprendizad de profundo son multiplicaciones de matrices eso pues lo lo reforzaremos cuando vamos a la red neuronales pero ese eso realmente las renornales no son nada más que multiplicaciones de matrices y composiciones con funciones no lineales eso es todo pero pues claro la gente al darse cuenta que estos chips incluyendo cerebro y lo habilitado hay mucho chips que están saliendo al mercado específicamente para hacer esas multiplicaciones de matrices rápidas es lo que diferencia en que usted tenga una empresa o un monopolio entonces la tecnología es súper importante pero ya en términos de aprendizad de profundo vas a saltar un poquito porque esto ya es de la otra sesión quisiera comenzar a darles en estos minutos que sean ejemplos ejemplos de qué es lo que está ocurriendo en este momento en la inteligencia artificial esto entonces es tan sencillo como que yo ya puedo tener aplicaciones de aprendizaje profundo como lo siguiente entonces aquí hay uno que se llama hyperjax y entonces yo puedo hacer cosas tan fácil como que con una red neuronal puede hacer traducción de texto a otro si ustedes lo quisiera podríamos en alguna sesión entrar directamente a una red neuronal que lo vamos a hacer en algunos casos para que vamos por dentro del repositor de github como es que están construidas esas redes por dentro para que ustedes entiendan a profundidad que ese código de donde salió porque puedo confiar en él entonces por ejemplo este es whisper es un modelo que me sirve para traducir audio y pues la primera tarea que le podía dejar además de leer el rol de cien de los científicos de datos de los roles es que ustedes tomen las grabaciones de estas tres horas hagan la transcripción con whisper y con ayuda chat gpt me pueden dar los puntos fundamentales de la clase de hoy ignorando la presentación de los estudiantes no bueno incluyan la incluyanla para que sepamos que lo que digo cada uno entonces la tarea es tomen las grabaciones de alguna manera invéntense como separar los videos de los audios los audios de los videos transcriban la información con whisper a que les doy una pista a esto es una pion line sino les funcionan entonces investiguen cómo podrían hacer funcionar whisper ya sea por colab ya sea por máquina local si hay posibilidades quiero que me cuenten qué soluciones vieron la siguiente sesión tomen esas transcripciones y hagan unos puntos claves de la clase de hoy y el que tenga los mejores resultados y los que exponga voy a dar 10 minutos para que vamos a exponer la siguiente clase de eso es libre no es obligatorio exponer pero la tarea así es obligatoria los mejores equipos que yo vea los máximios del primer quiz a las personas que me convenzan que esos equipos y pues bueno como yo hago esto adiario en mi empresa entonces mar cuenta cuando lo hizo vino cuando lo hizo bien entonces también la idea entonces es bueno entremos al código esto se ve súper en chino cierto pero pues la idea es ya con lo que ustedes tienen de Python y todo eso esto lo podemos entender un poquito mejor y en algunas ocasiones nos vamos a ir dentro de estos códigos para decir mire aquí es la parte clave del modelo aquí para que usted cuando lo utilice como caja negra puede entender qué es lo que está por dentro y lo puede usar con tranquilidad no lo vamos a hacer con todos al menos van a ser tarea otros no pero no nos vamos a centrar en código sino en las aplicaciones directas de la inteligencia de la princesa de profundo entonces whisper es un modelo que sacó por ahí o pene hay es libre usted puede hacer un producto empresarial con esto y básicamente es un transcriptor es un transcriptor de audio comentims que también creo que Google tiene lo mismo que te transcribe te transcribe en tiempo real lo que está diciendo en este caso por ejemplo si yo estoy entonces miremos un momentico si yo estoy hablando en este momento es porque es algo importante me cambien bien y se estudiante es gracias y que podamos hacer la transcripción o la traducción por ejemplo en este caso fíjense que se están usando algunos recursos en la nube y una de las aplicaciones más interesantes que yo he podido ver y que seguramente se va a hacer una tecnología que acaba de hacer fundamental ahorita en el presente y que se va a volver algo esencial en el futuro como integración de todo esto incluso chatgipit y la aplicación del celular tiene ese método que uno le habla y él responde también hablando entonces speech to speech te habla habla y fíjense la calidad de esos resultados si yo estoy hablando de cemento es porque es algo importante me cabe en mi casa en mi casa y se estudiante es gracias entonces un ejemplo súper básico con aprendizad de profundo es posible esto porque son redes neuronales profundas que vienen ya digamos si ustedes vieron este perceptron múltica y todas estas cosas muy seguramente si se darán cuenta que realmente lo que les decía no era no era mentira el perceptron múltica pasa de usa en nada y en todo el mismo tiempo porque resulta que esta arquitectura de whisper yo traduce con mi audio a un espectro grama solo lo vamos a ver porque lo va a poner el pedaquete es un espectro grama es una forma de ver el audio y comenzó a hacer bloques transformar y un bloque transformar por dentro no es nada más que una red neuronal profunda que por dentro tiene un perceptron múltica para entonces acá está escuipenme acá donde se fit forward esto es un perceptron múltica para profíjense que es parte fundamental intrínseca de un modelo pero usar un perceptron múltica para hoy día como modelo en sí podría hacer algo extraño porque no sirve para todas las aplicaciones o para cosas de más específicas y no tiene como un propósito es el retener neuronal pero cuando lo insertamos como piezas claves del ego en ciertas arquitecturas funciona la maravilla acá por ejemplo hay dos perceptrones múltica entonces estos bloques y dos son partes del whisper entonces súper interesante entonces la primer ejemplo de aprendizaje profundo que se sale todos los modelos estadísticos que hemos visto lo estamos viendo desde el punto de vista la aplicación whisper parte 1 parte 2 modelos de visión por computadora que hacen un sustamente retener neuronales pero con una tecnología que se llama las redes convolucionales las convoluciones perdón y hay muchísimos ejemplos entonces hay un estudiante por ejemplo que quiere hacer sus aplicaciones con inteligencia artificial con redes neuronales pero y eso es algo que también tienen que definir en sus proyectos el uso o el uso de la nube creo que en esa maestra hay una casi una turólica turía del uso de la nube me parece fantástico hay que definir cuando su empresa le deja sacar los datos es súper peligroso sacar datos sensibles a la nube si usted no sabe lo que está haciendo y que la empresa obviamente no quiere sacar en el sector público sos muy complicado en fiscal y a por ejemplo no nos dejan sacar nada con razones súper validas pues si llegan a jagear y eso pues todas las gente va a hacer todas las denuncias de todo el mundo es un caos entonces tomar esas decisiones es muy importante hay unos modelos de visión que son súper efectivos entonces vamos a intentar ver uno para los que no sepan donde encontrar estos modelos que uginfaces la plataforma por defecto donde están casi la mayoría de modelos que tienen que ver con transformers pero también se pueden encontrar modelos de visión y modelos otras cosas y para que se en una idea que uno no tiene en principio que hacer un modelo del de cero actualmente de 821 1113 modelos que están siendo jostiados que están siendo almacenados en uginfaces que se convirtió en un monopólio de hosting de modelos de inteligencia artificial entonces digamos quiero cambiarle ese chip es muy poco probable que les toca hacer un modelo del de cero primero no creo que tengamos el dinero porque hacer eso es muy complicado a menos que la aplicación lo amerite si harto si tengo ya mis datos de la empresa y son estos sino tengo más y no encontré un modelo que tenga unas características similares pues hay hay hay toca hacerlo si harto pero muy probablemente la mayoría de aplicaciones que veamos son tomé este modelo de acá a finelo con sus datos y ya tiene su producto personalizado a la medida entonces por ejemplo hay un modelo de visión muy interesante y que nos puede servir por ejemplo para dar un a un pequeño ejemplo entonces discúlpenme un momentico yo acá lo busco de lo que encuentre mis apuntes para exactamente darles el nombre del modelo entonces tablas se llama Florence Florence 2 entonces el Florent 2 es un modelo no pues sí que de aunque volverse un poquito experto en buscar pero digamos que para eso están en esa clase entonces el Florence 2 es un modelo que me permite detectar tablas detectar objetos dentro de dentro de los dentro de imágenes vamos a usar este lo que es que aquí había un comúndemito pero en este momento no lo estoy encontrando entonces discúlpenme yo me lo envío lo envío al grupo y ya lo vemos desde ahí el tiempo súper corto no les parece estas clases deberían ser como de más de más sobre pero el tiempo se nos va vamos a tratar de poner la aceleradora la próxima vez para que no no nos coja la noche ese es el étrito significa que ya quedan poquitos minutos y fíjense entonces es otra reteneronal Florence 2 y aquí yo básicamente puedo colocar una imagen no significa que usted tenga que que utilizarlo desde está pip puede utilizar el código y extraerlo entender y colocarlo en su aplicación a finarlo es la idea y entonces por ejemplo coloquemos por ejemplo esta imagen que va a tomar cierto y uno le puede decir hoy que a este modelo es un modelo de visión pero tiene integrada también parte del lenguaje natural y entonces puede extraer cosas ya y por ejemplo puede extraer en no sé de tección de objetos y entonces yo mando así como de tu ejemplo please tell me oh bueno digamos en español dime sobre todos los objetos presentes en la imagen muy importante esto la trasección veremos también ya unos modelos que son pues en la nube chatipitita también permite hacer este tipo de cosas pero entiendo que uno quisiera tener sus modelos de manera local entonces por ejemplo aquí pues estamos procesando obviamente estos son recursos que alguien hostio en higgin facebook probablemente pueda tener demoras por qué pues está colocando sus recursos propios y fíjense no el resultado acá hay una persona y hay un rostro humano y esto yo y esos modelos tienen pues como les digo diferentes tipos de tareas que uno podría utilizar pero esto lo estamos viendo desde el punto de vista de alguien que hizo el desplegue pero si nosotros nos fuéramos a higgin facebook les mostrarita podríamos ver ese modelo florence toca que veamos la licencia y simplemente miren aquí están los datos y el modelo 821 mil modelos uno se pone a preguntar será que es importante que hagan modelos de cero la respuesta depende cuál es su problema pero si es un problema que te queda con texto con vision muy probablemente no se ha lo indicado hacer lo desde cero a menos que es un problema super específico entonces digamos este es otro ejemplo que les quería mostrar ya vamos a tres ejemplos chat gpt whisper florence obviamente la nube tiene un modelo súper interesante a su tiene por ejemplo el document inteligencia es muy interesante para los que estén interesados en cero de tiempo entonces modelos de aprendizaje profundo hay unos modelos que ya están ya están un poquito no hay gobsoletos pero pues que en su momento tuvieron el auge estamos hablando de dos mil veinti tres dos mil veinti uno veintidós que salieron esos model bueno la primera versión salió en 2019 y son muy buenos para hacer precciones de cero de tiempo y también hacen uso de la aprendizaje profundo lo interesante es que aquí recuperamos la interpretación de las covaria entonces hay un mito que también quiero que también el chip de que dicen que las redes neuronales la aprendizaje profundo se pierde la interpretabilidad de los modelos y por lo tanto no son útiles y yo refuto muy siempre lo mismo con este tipo de ejemplos entonces este es el temporal fusion transformer nos permite hacer predicciones de cero de tiempo con interpretabilidad y siempre es general debate bueno usted quiere ver chat gpt tiene aproximadamente 400 billones de parámetros sobre todo está preguntado los estadísticos os le interesa saber quiso cada neurona tal vez no cierto porque si si si si si la aplicación me está dando la solución que yo necesito cierto una conversación cuidando yo puedo extraer la información de manera óptima a mí porque me interesaría saber qué dice cada neurona de la red sin embargo este es un píper muy interesante para la introducción entonces no digamos no vamos a como tal no era un tema específico más allá de lo que ya dijimos y es un paper de open AI como para que se vayan contentos de que hoy día ya somos capaces de recuperar la interpretabilidad casi todos los modelos de inteligencia artificial en aprendizaje profundo y el paper es language model can explain neurons language model los modelos del lenguaje pueden explicarle las neuronas en los modelos del lengualo súper cool y hay ejemplos entonces cuando chat gpt te da una respuesta el te puede decir exactamente qué neuronas activando y cuales la posible interpretación de esa neuronas utilizan el mismo modelo de lenguaje entonces les voy a mandar esto como para que se vayan felices y contentos de ciróica esto es re neuronal esta vez no sé si les se las he enseñado así que no dice no si necesitamos muchas máquinas para entrenar necesitan no hay interpretabilidad falso lo de baton esta primera clase palcio lo que pasa es que hay que estar actualizados para saber qué es lo que sí qué es lo que no entonces ese paper científico y el temporal fusion transformar son unas cosas bien interesantes que hay que tener en cuenta para cuando no vaya a debatir con expertos que está el hype de no es que esto no es explical entonces no me sirve entonces la pregunta la respuesta siempre depende que lo que está intentando hacer entonces es bien interesante esto el temporal fusion transformar no lo voy a colocar eso de pronto lo vamos a ver cuando veamos la red sdm como una aplicación del estado del arte para de otra red de prediction de bolsa que es mucho más interesante que se llama mamamba pero bueno lo veremos en su tiempo y como última aplicación de el aprendizaje profundo vamos a hacer en nuestro logo del grupo si les parece entonces el profesor alo si si si si el perfecto soy un profesor de la universidad central en la maestria de analítica de datos punto quisiera crear un logo para el grupo de mis estudiantes de whatsapp para motivarlos a que entiendan los alcances de el aprendizaje profundo junto generó un logo que sea muy original y que pueda sobrepasar una encuesta de satisfacción al final de la primera clase esto a bueno entre otras pues estoy aplicando el modelo whisper dentro de mi computador para poder hacer este este proceso entonces fíjense en el lugar que está en la que está la tecnología pero me dije me dijo que no entonces no pero hazme un logo con Dalí entonces no tengo la capacidad de generar que loco es que es el gp de cuatro exacto de así es no me salió de uno listo entonces con ese sí se puede o lo cambiaron intentemos hacer directamente con Dalí es muy extraño este modelo le han dejado multimultitasque pero bueno es cuestión de decirle que lo genera entonces pues les voy a dejar otra tarea para también que no sea pues solamente una persona que es exima del quiz generen el mejor logo que puedan con Dalí sino tienen acceso a ahí pues hay otras opciones o pensors busquenlas y sino pues me pueden pasar los prompts y yo puedo generarlas y definir y le comire tu prompt dio este resultado tu prompt dio este resultado tu promptio este resultado y hacemos una votación entre todos y miramos cuál es el mejor loco por ejemplo este loco no me gusta entonces vamos a cambiar entonces vamos a poner la instrucción es más interesante que sea muy artístico y que refleje un concepto abstracto de educación remota pero que tenga que ver con el mundo actuar esto que estamos haciendo en este momento es prompting cómo hacemos cómo hacemos para doblegar a estos modelos del lenguaje natural y que nos salen casos pues la verdad no me me mató a veces siento que con el modelo gpt4 que entra el módulo Dalí da muchos mejores resultados pues bueno yo también puedo hacer mi propuesta y ver pues como hacemos entonces a bueno acá es muy interesante le podemos decir que sea por ejemplo minimalista por ejemplo digamos que sea minimalista entonces que sea minimalista y ya con esto entonces daríamos cierre la sesión entiendo que ya quedan solamente 6 minuticos entonces pues aprovechemos de a 11 si algunos ustedes tiene alguna otra clase ya se podría retirar la idea es que siempre terminemos 10 minuticos antes pero pues yo me he enredo porque me gusta muchísimo este tema entonces puedo hablar por horas y horas y horas entonces no me gustó ningún logo voy a tratar de generar uno se los voy a proponer y luego miramos que nos gusta todos aquí yo vi uno un logo que está no sé si se fue generada con inteligencia artificial creo que no pero generen sus propuestas para facilidad del grupo no sé si se pueda paola generar una comunidad creo que ahorita en el receta se me está ocurriendo desde interesante y era generamos la comunidad de lo que es el el curso y dividamos los en diferentes chats por la diferente rama de la aprendizaje profundo que hoy les te mostré que es vision por computadora procesamiento del lenguaje natural sería el de tiempo pues ponemos lo como regresiones sí déjeme lo como sería el de tiempo esas tres ramitas y una que sea bueno aprendizaje reforzado pero pues esa no lo vamos a ver tanto en esta clase entonces que sean esas tres ramitas y otra que pues que sea este chat como tal para que podamos digamos en los diferentes aspectos de sus proyectos yo ir botando pues los papers vídeos interesantes pero entonces que no se combiné todo en un chat sino que pues podamos verlo por secciones yo sé que las comunidades pueden hacer pero pues si perdemos el chat entonces no lo hacemos entonces les dejo ese inquietud no sé si tengan alguna pregunta en estos cinco minuticos que nos quedan eso sería pues por mi parte el día de hoy la clase pues espero que les haya gustado y pues estoy atento sus a sus observaciones chicos y chicas un inquietud respecto al logon que propósito que tenemos que tengan logos a bajo criterios es idea que lo creemos al final vamos a hacer una encuesta entonces el logon que tenga mejor acogida por su estética porque refleje inspiración a los demás yo también votaré esas son y decir vendan el producto entonces no hay restricciones venden el producto si si quieren que sea artístico que no sé una silueta de pronto no es una foto que se parece que la universidad central o no sé esos son como los requisitos para al final la votación va a ser todos si cada nota por su logon pues hacemos que chat y piti pero es es sería como la la instrucción listo bueno listo un gusto estar con ustedes y esta grabación entonces la voy a estar subiendo a mi drive de manera personal y ya cuando me en acceso al corro institucional entonces vamos a subir las grabaciones el material pues si es el repositorio entonces no hay ninguna ninguna otra no los vamos a subir a ir porque se desactualizan muy rápido eso sería por mi parte para que hay si quieres no sé si quedó bien en la comunidad ahí está dentro del chat del grupo ha listo perfecto excelente entonces ahí vamos a comenzar las cositas listo muchas gracias paola muchas gracias chicos chicas nos vemos entonces dentro de ocho días dentro ocho desbacer si fuerte la clase va a ser ya al punto o en una introducción de la siguiente clase siempre preparado ya acódigo acositas pues ya más avanzan más específicas y pues vamos a comenzar a avanzar mi compromiso ya sería la el pd actualizado para que ya interacten los con el estado de las cosas listo que está en bien pero lo que tú dices como hacer si alguno me pueda verificar se perdió el grupo no está o ahí está la verdad si ahí está pero tú propieces que se repredimos el grupo bueno cualquier cosa estamos en comunicación por el grupo creo que la llamaba para automáticamente entonces un gusto conocerlos conocerlas entonces cualquier duda entre la semana interactuemos y el